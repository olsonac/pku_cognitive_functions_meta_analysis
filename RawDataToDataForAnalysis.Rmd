---
title: "Processing raw data to data for analysis report"
author: "Andrew Olson"
date: "22/04/2022"
output: html_document
knit: (function(inputFile, encoding) { 
      out_dir <- 'FunctionResults2';
      rmarkdown::render(inputFile,
                        encoding=encoding, 
                        output_file=file.path(dirname(inputFile), out_dir, 'RawDataToDataForAnalysis Report.html')) })
---
```{r setup}
# this script runs a meta-analysis for a single function (e.g. 'Flexibility and planning')
# NOTE:  change the "title:" above to the name of the measure you are analyzing
#        There are some similar variables that need to be set below so labels are correct
#        see the comment #  SET THE TASK TITLE HERE # below
#        and the comment for the plot filename in the same region

# the first time you may need to install the library 'metafor'
# run the following line after removing the '#' then add the '#' back once you've installed the package
# install.packages("metafor")

# added heterogeneity measure and pooled effect size (was just z-stat and p-value)

# load libraries 
library(metafor)
library(metaviz)
library(kableExtra)
library(tidyverse)
library(altmeta)

options(digits = 10) # make sure we have sufficient digits when writing .csv files 

# set the working directory to the folder with the data
# make this match the folder where you've put the files
CurDir<-"~/Documents/current/PKU/pku_cog_functions_meta_analysis/May2022_meta-analysis_with_added_papers"
knitr::opts_knit$set(root.dir=CurDir)

setwd(CurDir)
source("ForestPlotFunctions.R")
```

```{r}
OutlierSDCutoff <- 2.5
cat(sprintf("Using an outlier SD cutoff of %02.f\n",OutlierSDCutoff))
```


```{r}
# read a file with the column names in column 1 and whether to
# include the column in the data or not in column 2
ColNames<-read.csv("ColumnNames.csv")
head(ColNames)
```

```{r}
RawData<-read.table("/Users/Olsonac-admin/Documents/current/PKU/pku_cog_functions_meta_analysis/May2022_meta-analysis_with_added_papers/RAW DATA for META-ANALYSIS_June2022.csv",
                  encoding = "UTF-8",
                  sep=",",
                  header=FALSE,
                  skip=4,
                  fill=TRUE
                  )
RawData<-RawData[,1:length(ColNames$ColName)]
colnames(RawData)<-ColNames$ColName

# remove unwanted columns
RawData <- RawData %>% select(which(ColNames$Type != "NULL")) 

# remove blank or notes rows
RawDataNBL<-RawData[which((RawData$Function != "") & (RawData$StudyName != "")  & (!is.na(RawData$Year))),]

NonNullColNames<-ColNames[which(ColNames$Type != "NULL"),]
for(i in seq(1,length(NonNullColNames$Type))){
  FunctionString<-paste0("as.",NonNullColNames$Type[i])
  ConvertType<-get(FunctionString)
  RawDataNBL[,NonNullColNames$ColName[i]]<-ConvertType(RawDataNBL[,NonNullColNames$ColName[i]])
}
head(RawDataNBL)
```


```{r}
# read study factors and assign 
# StudyFactor marks separate articles that use the same population of
# participants

StudyFactorDF<-read.csv("StudyNameStudyFactor.csv")
StudyFactorDF<-StudyFactorDF[,1:2] # in case extra columns are read
head(StudyFactorDF)
```

```{r}
RawDataNBL<-left_join(RawDataNBL,StudyFactorDF,by="StudyName")
print("Missing study factors: ")
print(RawDataNBL[which(is.na(RawDataNBL$StudyFactor)),])
```

```{r, echo=FALSE, include=FALSE}
# copy PKUData into a data frame called 'MyData'
# this seems unnecessary, but will help if we wanted to adapt this to some new dataset
# later
MyData<-RawDataNBL

MyData$Function<-as.character(MyData$Function)

MyData<-MyData[MyData$Function!="Higher Language Skills Accuracy only",] # Phe incorrect
MyData<-MyData[MyData$Function!="Social Cognition Accuracy",] # Phe incorrect
MyData<-MyData[MyData$Function!= "Sustained and divided attention",]

# make short title shorter for Jahja/IQ
MyData$ShortTitle[MyData$ShortTitle == "WISC-III and WAIS III Vocabulary/Block Design"] <- "WISC-III and WAIS III Vocab/Block Design"
```

```{r, echo=FALSE, include=FALSE}
# below we calculate the standardised mean difference 

# variables are in the form:
# [data frame] $ [column name]  e.g. PKUData$ExpN refers to the PKU datq frame
# and the "ExpN" column (this has the N for the number of patients in a study)


MyData$CohensD<-((MyData$ControlMean-MyData$ExpMean)/sqrt(((MyData$ExpN-1)*MyData$ExpSD^2 + (MyData$ControlN-1)*MyData$ControlSD^2)/(MyData$ExpN + MyData$ControlN -2)))*MyData$Reversed
MyData$HedgesG<-MyData$CohensD * (1 - (3/(4*(MyData$ExpN + MyData$ControlN - 2)-1)))

CorrectZeroControlSD<-function(ControlSD,ControlMean){
  # return a very small fraction of the mean for the SD if the measured SD is zero
  if(is.na(ControlSD)){
    return(ControlSD)
  }
  if(ControlSD == 0){
    return(ControlMean/1000000)
  }
  else return(ControlSD)
}

MyData$OriginalControlSD<-MyData$ControlSD
MyData$ControlSD<-mapply(CorrectZeroControlSD,MyData$ControlSD,MyData$ControlMean)

MyData$SMD<- ifelse(MyData$ControlSD != 0,
                    ((MyData$ControlMean - MyData$ExpMean) * 
                       MyData$Reversed) / MyData$ControlSD,0) 
# This also reverses scales were lower scores = better (marked with -1 in the 'Reversed' column) 

# use control SD rather than pooled SD because patient SD may not equal control SD
# This is the variable 'd'  

# use input data SMD when mean/sd are missing

MyData$GlassDelta<-MyData$SMD
MyData$GlassDelta[which(is.na(MyData$ExpMean))]<-MyData$InputDataSMD[which(is.na(MyData$ExpMean))]

head(MyData)
```

```{r}
# Function table
MyData$Function<-ifelse(MyData$Function == "IQ*","IQ",MyData$Function)
# put IQ and IQ* together
FTable<-MyData %>% group_by(Function) %>% summarise(MeanCohensD=mean(CohensD,na.rm=TRUE), MeanHedgesG=mean(HedgesG,na.rm=TRUE), SDHedgesG=sd(HedgesG,na.rm=TRUE), MeanGlassDelta=mean(GlassDelta,na.rm=TRUE), N=n())

print(FTable)
```


```{r}
CalculateGVg<-function(ExpMean, ExpSD, ExpN, 
                       ControlMean, ControlSD, ControlN, 
                       Reversed, InputDataSMD, Tdiff, 
                       InputDataGlassDeltaSD){
###############################################
#
# calculate standarised mean difference -- see formulas in
# https://www.jepusto.com/alternative-formulas-for-the-smd/
# look at section "independent groups with different variances"

# calculate the scaled standard error squared
# this formula is in the section "Independent groups with different variances"
# we don't take the square root -- this is the value that is inside the square root
# i.e. the squared value -- note that the patient group has the subscript 'T' in
# the formulas in the article and the control groups has the subscript 'C'
  
  
# If SD is not available (i.e. is.na(ExpSD) == TRUE) but effect size is available
# also from James E. Pustejovsky
# https://www.jepusto.com/alternative-formulas-for-the-smd/
# using the formula with t, v=N_control - 1 and the small
# sample correction: g = data_g x (1-(3/(4*(N_control -1)-1)))
# Vg = (g^2/t^2 + g^2/2(Nc-1))
# df = Nc -1  
  
  SMD<-ifelse(!is.na(ExpSD), 
               ifelse(ControlSD != 0, # to avoid divide by zero
                    ((as.numeric(ControlMean) - as.numeric(ExpMean)) * as.numeric(Reversed)) /
                       as.numeric(ControlSD),
                    0),
               NA)
  
  ScaledSESqr<- ifelse(!is.na(ExpSD),
                       ifelse(ControlSD != 0, # to avoid divide by zero
                              (ExpSD^2/(ControlSD^2 * ExpN)) + (1/ControlN),
                              0),
                       NA)
  # note - no square root because we would
  # just square this again later
  
  # calculate the degrees of freedom -- this is 'v = Nc-1' in the article
  Vdf<-as.numeric(ControlN)-1

  # this a correction for small samples -- this is J in the article, with
  # Nc-1 as the argumnent to the function.  J is defined in the section
  # titled "SMD from a simple independent groups design" - J(x) = 1 - (3/(4x-1))
  # x is equal to Nc-1
  J<-1-(3/(4*Vdf-1)) # J(Nc-1) from article

  # multiply the SMD by the small sample correction to get the effect size
  # In the article the raw effect difference 'd' is multiplied by 'J' to 
  # get 'g', which is the bias corrected estimator of the standardised mean difference
  g_value <- ifelse(!is.na(ExpSD), 
                    SMD*J,  # bias-corrected estimator of standardised mean difference
                    J*InputDataSMD)

  # sampling variance of g, the bias corrected estimate of effect size
  # this is from the section "A general formula for  g and its sampling variance"
  # where the formula is Vd = (SEb/S)^2 + (d^2/2V)
  # SEb/S is the value above that we didn't take the square root of, 
  # so we don't square it here.  We use 'g' as the bias corrected estimate instead
  # of d (as noted in article).  V is calculated above (Vdf)
  Vg<-ifelse(!is.na(ExpSD),
             ScaledSESqr + ((g_value^2)/(2*Vdf)),
             InputDataGlassDeltaSD)
  return(c(ScaledSESqr,Vdf,J,g_value,Vg))
}

CalculatedGVg<-matrix(
  do.call(function(ExpMean, ExpSD, ExpN, 
                   ControlMean, ControlSD, ControlN,
                   Reversed, InputDataSMD, Tdiff,
                   InputDataGlassDeltaSD,...) CalculateGVg(
                      ExpMean, ExpSD, ExpN, 
                      ControlMean, ControlSD, ControlN, 
                      Reversed, InputDataSMD, Tdiff,
                      InputDataGlassDeltaSD),
          MyData),
  nrow=length(MyData[,1]))

MyDataWithG<-cbind(MyData,CalculatedGVg)
names(MyDataWithG)<-c(names(MyData),"ScaledSESqr","Vdf","SmallSampCor","g","Vg")

```


```{r}
DataNADF<-MyDataWithG[is.na(MyDataWithG$g),]
if(nrow(DataNADF) > 0){
  cat("*** WARNING *** Entries below have missing entries for glass delta: \n")
  DataNADF %>% kable() %>% kable_styling()
}

```


```{r}
head(MyDataWithG)
```

```{r}
# mark outliers
MyDataWithG$g <- as.numeric(as.character(MyDataWithG$g))
MyDataWithG <- MyDataWithG %>% group_by(Function) %>% mutate(FunctionMean = mean(g), FunctionSD = sd(g))

MyDataWithG$OutlierLowCut<-MyDataWithG$FunctionMean - (MyDataWithG$FunctionSD * OutlierSDCutoff)
MyDataWithG$OutlierHighCut<-MyDataWithG$FunctionMean +  (MyDataWithG$FunctionSD * OutlierSDCutoff)
MyDataWithG$Outlier <- 0
MyDataWithG$Outlier[(MyDataWithG$g < MyDataWithG$OutlierLowCut) | (MyDataWithG$g > MyDataWithG$OutlierHighCut)]<-1
```

```{r}

OutlierDF <- MyDataWithG[MyDataWithG$Outlier == 1,]
cat(sprintf("The number of outliers was: %d\n",nrow(OutlierDF)))
cat("Outliers:\n")
OutlierDF %>% kable() %>% kable_styling()
```



```{r}
MyDataWithG <- MyDataWithG[MyDataWithG$Outlier == 0,]

write.csv(MyDataWithG,"/Users/Olsonac-admin/Documents/current/PKU/pku_cog_functions_meta_analysis/May2022_meta-analysis_with_added_papers/DataForMetaAnalysis.csv",row.names = FALSE)
```

